<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>第一篇 96G显存的4060？简单聊下AI MAX 395 | My New Hugo Site</title>
<meta name="keywords" content="">
<meta name="description" content="之前 5090？Project DIGITS？Ryzen AI Max&#43; 395？有哪些想买的AI产品 简单聊过 AI MAX 395 这个APU。
作为AMD的第一代核显性能和独显打平的APU，我个人还是非常感兴趣，于是斥巨资买了一台幻x2025款。
因为不确定其除了LLM能力（绝大部分宣传稿提到的），在通用AI领域能力相比4060版本怎么样（比如生图、生成视频、跑各种AI库等等），所以买了个丐版先尝尝鲜。相比于395版本的满血40CU，丐版390的显卡核心为32CU，理论性能相差20%。

390-8050s
AI MAX 395满血TDP是120w左右，幻x只有90w手动模式，无法完全释放性能，而且有溢价。
所以真正比较实惠的还是买MINI主机版本，现在有很多厂商下场在做了，包括前几天已经开始发售的极摩客evo-x2，未来一两个月还会有fevm、零刻、玲珑等厂商做mini主机，这个性价比会高些。
RTX 4060的性能
我们先看下4060的性能，4060比较特殊，笔记本和桌面端除了功耗上限外，硬件配置基本一致。
按照官方展示的算力来计算，列两个关键的指标：

Tensor Core fp16算力，非稀疏，60.5 TFlops ，换算成FP8的话，翻个倍，而Cuda Core fp16的算力为 15.11 TFLOPS
带宽 272g/s 、TGP为115W


因为FP16是最常用的精度，就以FP16为准。虽然实际中tensor core和cuda core可以同时执行，但是理论算力不可能叠加（因为每个sm的资源限制，一般来说跑tensor core就没资源跑cuda core），所以这里按照4060最大tensor core算力来算，也就是60.5Tflops。

当然tensor core的适用性不如cuda core，因为目前现在大部分AI任务都是基于矩阵乘法，所以可以近似地按照这个算力来估算
AI MAX 395 / 390 介绍
AI MAX 395的核显为8060s和我这个丐版390的核显8050s，两者代号都为gfx1151，FP16算力分别是60 tflops 和45 tflops。
可以看到8060s的fp16算力基本和4060的fp16算力相当。
理论算力怎么来的
简单分析下，因为8060s基于RDNA3.5架构，和RDNA3的RX 7900架构基本一致，所以直接借用RDNA3的数据来分析：

8060s架构和RX7900基本一致
通过上表可以得到，核显中的 CU 每个周期可以执行 512 次fp16/bf16/INT8的乘加操作，1024次INT4的乘加操作。在最大时钟频率 2.9GHz 下，其峰值性能应为 59.4 FP16/BF16 TFLOPS，通过这个公式可以计算出来，接近60TFLOPS，和4060相当。">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/ai-max-395-1/ai-395-max-1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/ai-max-395-1/ai-395-max-1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="My New Hugo Site (Alt + H)">My New Hugo Site</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      第一篇 96G显存的4060？简单聊下AI MAX 395
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-07-06 17:24:12 +0800 CST'>July 6, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>之前 <a href="https://mp.weixin.qq.com/s/IYgRPWDQ52UW6OXK7B-jQg">5090？Project DIGITS？Ryzen AI Max+ 395？有哪些想买的AI产品</a> 简单聊过 AI MAX 395 这个APU。</p>
<p>作为AMD的第一代核显性能和独显打平的APU，我个人还是非常感兴趣，于是斥巨资买了一台幻x2025款。</p>
<p>因为不确定其除了LLM能力（绝大部分宣传稿提到的），在通用AI领域能力相比4060版本怎么样（比如生图、生成视频、跑各种AI库等等），所以买了个丐版先尝尝鲜。相比于395版本的满血40CU，丐版390的显卡核心为32CU，理论性能相差20%。</p>
<p><img alt="390-8050s" loading="lazy" src="image.png"></p>
<p>390-8050s</p>
<p>AI MAX 395满血TDP是120w左右，幻x只有90w手动模式，无法完全释放性能，而且有溢价。</p>
<p>所以真正比较实惠的还是买MINI主机版本，现在有很多厂商下场在做了，包括前几天已经开始发售的极摩客evo-x2，未来一两个月还会有fevm、零刻、玲珑等厂商做mini主机，这个性价比会高些。</p>
<h1 id="rtx-4060的性能">RTX 4060的性能<a hidden class="anchor" aria-hidden="true" href="#rtx-4060的性能">#</a></h1>
<p>我们先看下4060的性能，4060比较特殊，笔记本和桌面端除了功耗上限外，硬件配置基本一致。</p>
<p>按照官方展示的算力来计算，列两个关键的指标：</p>
<ul>
<li>Tensor Core fp16算力，非稀疏，60.5 TFlops ，换算成FP8的话，翻个倍，而Cuda Core fp16的算力为 15.11 TFLOPS</li>
<li>带宽 272g/s 、TGP为115W</li>
</ul>
<p><img alt="image.png" loading="lazy" src="image%201.png"></p>
<p>因为FP16是最常用的精度，就以FP16为准。虽然实际中tensor core和cuda core可以同时执行，但是理论算力不可能叠加（因为每个sm的资源限制，一般来说跑tensor core就没资源跑cuda core），所以这里按照4060最大tensor core算力来算，也就是60.5Tflops。</p>
<blockquote>
<p>当然tensor core的适用性不如cuda core，因为目前现在大部分AI任务都是基于矩阵乘法，所以可以近似地按照这个算力来估算</p></blockquote>
<h1 id="ai-max-395--390-介绍">AI MAX 395 / 390 介绍<a hidden class="anchor" aria-hidden="true" href="#ai-max-395--390-介绍">#</a></h1>
<p>AI MAX 395的核显为8060s和我这个丐版390的核显8050s，两者代号都为gfx1151，FP16算力分别是60 tflops 和45 tflops。</p>
<p>可以看到8060s的fp16算力基本和4060的fp16算力相当。</p>
<h2 id="理论算力怎么来的">理论算力怎么来的<a hidden class="anchor" aria-hidden="true" href="#理论算力怎么来的">#</a></h2>
<p>简单分析下，因为8060s基于RDNA3.5架构，和RDNA3的RX 7900架构基本一致，所以直接借用RDNA3的数据来分析：</p>
<p><img alt="8060s架构和RX7900基本一致" loading="lazy" src="image%202.png"></p>
<p>8060s架构和RX7900基本一致</p>
<p>通过上表可以得到，核显中的 CU 每个周期可以执行 512 次fp16/bf16/INT8的乘加操作，1024次INT4的乘加操作。在最大时钟频率 2.9GHz 下，其峰值性能应为 59.4 FP16/BF16 TFLOPS，通过这个公式可以计算出来，接近60TFLOPS，和4060相当。</p>
<pre tabindex="0"><code>512 ops/clock/CU * 40 CU * 2.9e9 clock  / 1e12 = 59.392 FP16 TFLOPS
</code></pre><p>既然算力相当，那我们实际测测性能如何？</p>
<h1 id="ai-max-395-核显分析">AI MAX 395 核显分析<a hidden class="anchor" aria-hidden="true" href="#ai-max-395-核显分析">#</a></h1>
<p>我这里的机器是8050s的幻x，安装fedora系统（另一个Linux发行版）为了更好实验。</p>
<p>然后基于这个仓库 <a href="https://github.com/ROCm/TheRock">https://github.com/ROCm/TheRock</a> 去搭建rocm + HIP + pytorch环境，关于gfx1151有大佬已经提交过了fix代码，基本可以跑起来，还有些小bug，但不影响测试 按照 <a href="https://github.com/ROCm/TheRock/discussions/244">https://github.com/ROCm/TheRock/discussions/244</a> 的步骤依次先执行构建命令：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span> docker buildx build  --build-arg AMDGPU_TARGETS<span style="color:#f92672">=</span>gfx1151 -f dockerfiles/pytorch-dev/pytorch_dev_ubuntu_24.04.Dockerfile . --load  -t pytorch-rocm:v1
</span></span></code></pre></div><p>构建完后，使用该命令启动容器:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo docker run --device /dev/kfd --device /dev/dri  --security-opt seccomp<span style="color:#f92672">=</span>unconfined  -it pytorch-rocm:v1 bash
</span></span></code></pre></div><p>启动后pytorch已经安装好了，这里安装了前几周release的2.7版本，在强制开启HIPBLASLT后：</p>
<p><code>export TORCH_BLAS_PREFER_HIPBLASLT=1</code></p>
<p>我们测试下极限性能，使用 <a href="https://github.com/stas00/ml-engineering/tree/master/compute/accelerator">https://github.com/stas00/ml-engineering/tree/master/compute/accelerator</a>提供的脚本测试：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>** Dtype: torch.bfloat16
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>** Platform/Device info:
</span></span><span style="display:flex;"><span>Linux fe5b1b32a344 6.14.3-101.bazzite.fc42.x86_64 <span style="color:#75715e">#1 SMP PREEMPT_DYNAMIC Wed Apr 23 13:07:40 UTC 2025 x86_64 x86_64</span>
</span></span><span style="display:flex;"><span>_CudaDeviceProperties<span style="color:#f92672">(</span>name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;AMD Radeon Graphics&#39;</span>, major<span style="color:#f92672">=</span>11, minor<span style="color:#f92672">=</span>5, gcnArchName<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gfx1151&#39;</span>, total_memory<span style="color:#f92672">=</span>11828MB, multi_processor_count<span style="color:#f92672">=</span>16, uuid<span style="color:#f92672">=</span>58580000-0000-0000-0000-000000000000, L2_cache_size<span style="color:#f92672">=</span>2MB<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>** Critical software versions:
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">=</span>2.7.0a0+git6537fd5
</span></span><span style="display:flex;"><span>hip<span style="color:#f92672">=</span>6.5.25172-b42a9c664, cuda<span style="color:#f92672">=</span>None
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>** Additional notes:
</span></span><span style="display:flex;"><span>benchmark version: <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>--------------------------------------------------------------------------------
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Warming up the accelerator <span style="color:#66d9ef">for</span> <span style="color:#ae81ff">30</span> secs ... rocblaslt info: HIPBLASLT_TENSILE_LIBPATH not set: Using /opt/rocm/lib/hipblaslt/library
</span></span><span style="display:flex;"><span>accelerator warmup finished
</span></span><span style="display:flex;"><span>^C^C
</span></span><span style="display:flex;"><span>Tried  <span style="color:#ae81ff">879</span> shapes <span style="color:#f92672">=</span>&gt; the best outcomes were:
</span></span><span style="display:flex;"><span>mean:   22.3 TFLOPS @ 3072x3072x1024 <span style="color:#f92672">(</span>MxNxK<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>median: 22.3 TFLOPS @ 3072x3072x1024 <span style="color:#f92672">(</span>MxNxK<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>max:    22.9 TFLOPS @ 3072x3072x1024 <span style="color:#f92672">(</span>MxNxK<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>geomean: 16.5 TFLOPS <span style="color:#66d9ef">for</span> <span style="color:#ae81ff">879</span> shapes in range: m<span style="color:#f92672">=[</span>0, 16384, 1024<span style="color:#f92672">]</span> | n<span style="color:#f92672">=[</span>0, 16384, 1024<span style="color:#f92672">]</span> | k<span style="color:#f92672">=[</span>0, 16384, 1024<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Legend: TFLOPS <span style="color:#f92672">=</span> 10**12 FLOPS
</span></span><span style="display:flex;"><span>Elapsed time: 0:49:39
</span></span></code></pre></div><p>测试的结果为23 Tflops，算是实际算力能达到的一个上限，8050s理论上为46tflops，达到了理论算力的 23/46=50%，</p>
<p>kernel优化AMD的有些差，作为对比，NV的cublas可以轻松达到理论算力的80%。</p>
<p>我这里的390并不是满血的核显，同时幻x的测试功耗从一开始的80w稳定到后来的65w也不是完全释放。那么满血的8060s性能如何，我们先简单估算下，首先需要确认性能提升是否和功耗成正相关，首先看下AI MAX 395这个APU的核显和功耗曲线图，可以看到核显从45w到120w都有性能提升，随着功耗越高、提升的幅度越小：</p>
<p><img alt="来自小明和他的女朋友的评测" loading="lazy" src="image%203.png"></p>
<p>来自小明和他的女朋友的评测</p>
<p>按照上文得到的两个核显的算力结论：</p>
<ul>
<li><strong>AI MAX 390</strong> 核显功耗 65 W 时测得 23 TFLOPS，</li>
<li><strong>AI MAX 395</strong> CU 数从 32 增至 40，理论上提升约 20%，</li>
<li>AI MAX 395 目标功耗：120 W</li>
</ul>
<p>我们可以通过建模的方式来推算下，因为性能随功耗并非线性增长，通常可近似建模为 $\text{Perf} \propto P^k,\quad k&lt;1$，因为这里的经验中，k 多在 0.6–0.8 之间。</p>
<p>我们假设：</p>
<p>$F_{390}(65) = 23;\mathrm{TFLOPS}$</p>
<p>$F_{395}(120) \approx F_{390}(65)\times 1.20 \times \Bigl(\tfrac{120}{65}\Bigr)^k$</p>
<ul>
<li>1.20：CU 增加带来的理论提升</li>
<li>$\bigl(120/65\bigr)^k$：功耗提升的亚线性增益</li>
</ul>
<p>不同k值下</p>
<table>
  <thead>
      <tr>
          <th>k <strong>值</strong></th>
          <th>$\bigl(\tfrac{120}{65}\bigr)^k$</th>
          <th><strong>估算 TFLOPS =</strong> $23\times1.20\times(120/65)^k$</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0.6</td>
          <td>$\approx1.359$</td>
          <td>$23\times1.20\times1.359\approx37.5 TFLOPS$</td>
      </tr>
      <tr>
          <td>0.7</td>
          <td>$\approx1.536$</td>
          <td>$23\times1.20\times1.536\approx42.4 TFLOPS$</td>
      </tr>
      <tr>
          <td>0.8</td>
          <td>$\approx1.737$</td>
          <td>$23\times1.20\times1.737\approx47.9 TFLOPS$</td>
      </tr>
  </tbody>
</table>
<ul>
<li>若 k 取中间值 ~0.7，则大约 40 TFLOPs，保守点就是37.5 TLOPs。</li>
<li>考虑系统其他开销（比如功率峰值处效率进一步下降，也就是上述图中到了90w核显的性能提升不明显了），取 0.6 左右范围更保守：也就是35-37 Ftops</li>
</ul>
<p>所以在 120 W 峰值功耗下，AI MAX 395 的核显实际 FP16 TFLOPS 预计约 35-37 TFLOPS，典型可取 ≈36 TFLOPS。</p>
<hr>
<p>不过正好B站UP主玲珑和秋月也测试了他们家的mini主机AI MAX 395系列，给出了一个数据36.2 Tflops，相比我这里的23 Tflops有 57%的性能提升，<a href="https://www.reddit.com/r/ROCm/comments/1kn2sa0/amd_strix_halo_ryzen_ai_max_395_gpu_llm/">另一个国外大佬测试出来是36.9Tflops</a>，和上述的估算基本一致。</p>
<p><img alt="image.png" loading="lazy" src="image%204.png"></p>
<p>而4060的FP16理论算力是60.6 tflops，同时这个算力是Tensor Core算力，相比cuda core来说不是很通用，再算上实际kernel性能折损，打个折，也就和8060s核显的算力差不多了。</p>
<h2 id="8060s支持的精度">8060s支持的精度<a hidden class="anchor" aria-hidden="true" href="#8060s支持的精度">#</a></h2>
<blockquote>
<p>While RDNA 3 doesn&rsquo;t include dedicated execution units for AI acceleration like the Matrix Cores found in AMD&rsquo;s compute-focused <a href="https://en.wikipedia.org/wiki/CDNA_(microarchitecture)">CDNA</a> architectures, the efficiency of running inference tasks on <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">FP16</a> execution resources is improved with Wave MMA (<a href="https://en.wikipedia.org/wiki/Matrix_multiplication">matrix</a> <a href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation">multiply–accumulate</a>) instructions. This results in increased inference performance compared to RDNA 2.<a href="https://en.wikipedia.org/wiki/RDNA_3#cite_note-tomshw-rdna3-15">[15]</a><a href="https://en.wikipedia.org/wiki/RDNA_3#cite_note-tomshw-interview-16">[16]</a> WMMA supports FP16, BF16, INT8, and INT4 data types.<a href="https://en.wikipedia.org/wiki/RDNA_3#cite_note-wmma-gpuopen-17">[17]</a> <a href="https://en.wikipedia.org/wiki/Tom%27s_Hardware"><em>Tom&rsquo;s Hardware</em></a> found that AMD&rsquo;s fastest RDNA 3 GPU, the RX 7900 XTX, was capable of generating 26 images per minute with <a href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a>, compared to only 6.6 images per minute of the RX 6950 XT, the fastest RDNA 2 GPU.<a href="https://en.wikipedia.org/wiki/RDNA_3#cite_note-tomshw-sd-18">[18]</a></p></blockquote>
<p>因为8060s没有像4060那样有tensor core，所以有一些精度不支持，也没有像CDNA那样的专用AI加速单元（Matrix Cores）。</p>
<p>不过8060s可以通过Wave MMA（矩阵乘累加）指令提升了FP16运算效率，支持的数据类型包括FP16、BF16、INT8和INT4，比较细节的是，这里的INT8算力是和FP16一样的，而INT4的算力是FP16的两倍，有点奇怪。</p>
<h2 id="目前已知的一些情况">目前已知的一些情况<a hidden class="anchor" aria-hidden="true" href="#目前已知的一些情况">#</a></h2>
<p>不要高兴的太早，AMD目前的适配情况相比NVIDIA还是差很多滴：</p>
<ul>
<li>Pytorch和一部分基于pytorch的库可以跑通（比如transformers和triton），但是实际中有很多bug…</li>
<li>目前HIP 的 matmul 操作默认使用 rocBLAS，而非 hipBLASLt，所以rocBLAS 在 gfx1151 上表现非常糟糕，解决方案是设置环境变量 <code>export ROCBLAS_USE_HIPBLASLT=1</code> ，我上述测试的时候 <code>TORCH_BLAS_PREFER_HIPBLASLT</code> 开启也是这个原因</li>
<li><strong>WMMA</strong> 或 <strong>Wave32 VOPD</strong> 必须启用才能达到峰值，否则性能会减半，通用性不是很强。</li>
<li>带宽测试峰值 <strong>212 GB/s</strong>，接近 DDR5-8000 256-bit 总线的理论峰值 <strong>256 GB/s</strong>。</li>
<li>CPU 到 GPU 的传输速率约 <strong>84 GB/s</strong>。</li>
</ul>
<p><img alt="image.png" loading="lazy" src="image%205.png"></p>
<p>大模型推理vllm和一些生图生成视频的模型还没有测，等之后mini主机到了再测，算是未完待续。</p>
<h1 id="结论省流版本">结论省流版本<a hidden class="anchor" aria-hidden="true" href="#结论省流版本">#</a></h1>
<p>AI MAX 395的这个8060s核显，在最大TDP下的算力和RTX4060差不多，而且可以自定义超过110g的显存（在ubuntu系统下）。</p>
<p>不过就是软件适配比较差，HIP相比NVIDIA的cuda差的很远。所幸有其他的后端可以使用（Vulkan后端的性能接近M4 Max的表现），目前来说这个很适合搞AI的开发者去玩一玩。</p>
<p>对我来说这个相当于一个可以设置100g显存的、支持fp16精度（int8和int4虽然支持但是实际中不是很好用）的4060，软件上开发的不够完善需要自己折腾，如果折腾好了潜力还是蛮大的。</p>
<h1 id="参考">参考<a hidden class="anchor" aria-hidden="true" href="#参考">#</a></h1>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/640255727">https://zhuanlan.zhihu.com/p/640255727</a></li>
<li><a href="https://llvm.org/docs/AMDGPUUsage.html#memory-model-gfx12">https://llvm.org/docs/AMDGPUUsage.html#memory-model-gfx12</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/631651468?utm_source=chatgpt.com">https://zhuanlan.zhihu.com/p/631651468?utm_source=chatgpt.com</a></li>
<li><a href="https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html">https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/RDNA_3#Integrated_graphics_processing_units_(iGPUs)">https://en.wikipedia.org/wiki/RDNA_3#Integrated_graphics_processing_units_(iGPUs)</a></li>
<li><a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU?tab=readme-ov-file">https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU?tab=readme-ov-file</a></li>
<li><a href="https://docs.docker.com/engine/install/fedora/">https://docs.docker.com/engine/install/fedora/</a></li>
<li><a href="https://docs.bazzite.gg/Installing_and_Managing_Software/rpm-ostree/">https://docs.bazzite.gg/Installing_and_Managing_Software/rpm-ostree/</a></li>
<li><a href="https://lanoc.org/review/video-cards/asus-dual-rtx-4060-8gb">https://lanoc.org/review/video-cards/asus-dual-rtx-4060-8gb</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/640255727?utm_source=chatgpt.com">https://zhuanlan.zhihu.com/p/640255727?utm_source=chatgpt.com</a></li>
<li><a href="https://discuss.pytorch.org/t/slow-fp16-gemm-on-4090/200232">https://discuss.pytorch.org/t/slow-fp16-gemm-on-4090/200232</a></li>
<li><a href="https://gist.github.com/stefansundin/fa1c1dd7a60ebe2f8a2aa6d32631b119?utm_source=chatgpt.com">https://gist.github.com/stefansundin/fa1c1dd7a60ebe2f8a2aa6d32631b119?utm_source=chatgpt.com</a></li>
<li><a href="https://www.nvidia.com/en-sg/geforce/graphics-cards/compare/">https://www.nvidia.com/en-sg/geforce/graphics-cards/compare/</a></li>
<li><a href="https://www.answeroverflow.com/m/1316221468581036055">https://www.answeroverflow.com/m/1316221468581036055</a></li>
<li><a href="https://blog.machinezoo.com/Running_Ollama_on_AMD_iGPU">https://blog.machinezoo.com/Running_Ollama_on_AMD_iGPU</a></li>
<li><a href="https://www.hardware-corner.net/how-fast-ai-max-395-llm-20250317/">https://www.hardware-corner.net/how-fast-ai-max-395-llm-20250317/</a></li>
<li><a href="https://www.phoronix.com/forums/forum/linux-graphics-x-org-drivers/open-source-amd-linux/1466910-linux-6-10-improves-amd-rocm-compute-support-for-small-ryzen-apus?utm_source=chatgpt.com">https://www.phoronix.com/forums/forum/linux-graphics-x-org-drivers/open-source-amd-linux/1466910-linux-6-10-improves-amd-rocm-compute-support-for-small-ryzen-apus?utm_source=chatgpt.com</a></li>
<li><a href="https://www.zhihu.com/question/1887429972517438037/answer/130528712443">https://www.zhihu.com/question/1887429972517438037/answer/130528712443</a></li>
<li><a href="http://reddit.com/r/ROCm/comments/1kn2sa0/amd_strix_halo_ryzen_ai_max_395_gpu_llm/">http://reddit.com/r/ROCm/comments/1kn2sa0/amd_strix_halo_ryzen_ai_max_395_gpu_llm/</a></li>
<li><a href="http://reddit.com/r/ROCm/comments/1kn2sa0/amd_strix_halo_ryzen_ai_max_395_gpu_llm/">reddit.com/r/ROCm/comments/1kn2sa0/amd_strix_halo_ryzen_ai_max_395_gpu_llm/</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">My New Hugo Site</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
